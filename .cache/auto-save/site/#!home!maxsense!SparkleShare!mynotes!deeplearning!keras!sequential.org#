#+startup: showall
#+title: keras 顺序模型

* 配置org babel使得pyenv生效
#+begin_src emacs-lisp
(setq org-babel-python-command "~/.pyenv/shims/python")
#+end_src

#+RESULTS:
: ~/.pyenv/shims/python

* 基于多层感知器的二分类sigmoid
#+BEGIN_SRC python :results output
#!/usr/bin/python
# -*- coding: utf-8 -*-
import numpy as np
np.random.seed(1337)  
from keras.models import Sequential 
from keras.layers import Dense,Dropout
import matplotlib.pyplot as plt

# 对于具有 2 个类的单输入模型（二进制分类）：
model = Sequential()
model.add(Dense(32, activation='relu', input_dim=100))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='rmsprop',
              loss='binary_crossentropy',
              metrics=['accuracy'])
model.summary()
# 生成虚拟数据
data = np.random.random((1000, 100))
labels = np.random.randint(2, size=(1000, 1))
x_test = np.random.random((100, 100))
y_test = np.random.randint(2, size=(100, 1))
# print(x_test,y_test)
# 训练模型，以 32 个样本为一个 batch 进行迭代
model.fit(data, labels, epochs=10, batch_size=128)
score = model.evaluate(x_test, y_test, batch_size=64)
print(score)
#+END_SRC

#+RESULTS:
#+begin_example
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_1 (Dense)              (None, 32)                3232      
_________________________________________________________________
dropout_1 (Dropout)          (None, 32)                0         
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 33        
=================================================================
Total params: 3,265
Trainable params: 3,265
Non-trainable params: 0
_________________________________________________________________
Epoch 1/10

 128/1000 [==>...........................] - ETA: 4s - loss: 0.9892 - acc: 0.4141
1000/1000 [==============================] - 1s 673us/step - loss: 0.8398 - acc: 0.4760
Epoch 2/10

 128/1000 [==>...........................] - ETA: 0s - loss: 0.7299 - acc: 0.5469
1000/1000 [==============================] - 0s 14us/step - loss: 0.7480 - acc: 0.5090
Epoch 3/10

 128/1000 [==>...........................] - ETA: 0s - loss: 0.7476 - acc: 0.4766
1000/1000 [==============================] - 0s 12us/step - loss: 0.7473 - acc: 0.4840
Epoch 4/10

 128/1000 [==>...........................] - ETA: 0s - loss: 0.6947 - acc: 0.5938
1000/1000 [==============================] - 0s 13us/step - loss: 0.7167 - acc: 0.5210
Epoch 5/10

 128/1000 [==>...........................] - ETA: 0s - loss: 0.7024 - acc: 0.5156
1000/1000 [==============================] - 0s 12us/step - loss: 0.7110 - acc: 0.5390
Epoch 6/10

 128/1000 [==>...........................] - ETA: 0s - loss: 0.7006 - acc: 0.5078
1000/1000 [==============================] - 0s 13us/step - loss: 0.7109 - acc: 0.4750
Epoch 7/10

 128/1000 [==>...........................] - ETA: 0s - loss: 0.6876 - acc: 0.5781
1000/1000 [==============================] - 0s 13us/step - loss: 0.6960 - acc: 0.5130
Epoch 8/10

 128/1000 [==>...........................] - ETA: 0s - loss: 0.6693 - acc: 0.5781
1000/1000 [==============================] - 0s 13us/step - loss: 0.6937 - acc: 0.5350
Epoch 9/10

 128/1000 [==>...........................] - ETA: 0s - loss: 0.6994 - acc: 0.4922
1000/1000 [==============================] - 0s 13us/step - loss: 0.6960 - acc: 0.4990
Epoch 10/10

 128/1000 [==>...........................] - ETA: 0s - loss: 0.6922 - acc: 0.5234
1000/1000 [==============================] - 0s 13us/step - loss: 0.6924 - acc: 0.5060

 64/100 [==================>...........] - ETA: 0s
100/100 [==============================] - 0s 256us/step
[0.6923628711700439, 0.4799999988079071]
#+end_example


* softmax多分类分类
#+BEGIN_SRC python :results output
#!/usr/bin/python
# -*- coding: utf-8 -*-
import numpy as np
np.random.seed(1337)  
import keras
from keras.models import Sequential 
from keras.layers import Dense
import matplotlib.pyplot as plt

# 对于具有 10 个类的单输入模型（多分类分类）：
model = Sequential()
model.add(Dense(32, activation='relu', input_dim=100))
model.add(Dense(10, activation='softmax'))
model.compile(optimizer='rmsprop',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# 生成虚拟数据
data = np.random.random((1000, 100))
labels = np.random.randint(10, size=(1000, 1))

# 将标签转换为分类的 one-hot 编码
one_hot_labels = keras.utils.to_categorical(labels, num_classes=10)

# 训练模型，以 32 个样本为一个 batch 进行迭代
model.fit(data, one_hot_labels, epochs=10, batch_size=32)
model.evaluate()
#+END_SRC

#+RESULTS:
#+begin_example
Epoch 1/10

  32/1000 [..............................] - ETA: 19s - loss: 2.3744 - acc: 0.1562
1000/1000 [==============================] - 1s 673us/step - loss: 2.3529 - acc: 0.1180
Epoch 2/10

  32/1000 [..............................] - ETA: 0s - loss: 2.3048 - acc: 0.0938
1000/1000 [==============================] - 0s 44us/step - loss: 2.3125 - acc: 0.1130
Epoch 3/10

  32/1000 [..............................] - ETA: 0s - loss: 2.2258 - acc: 0.1875
1000/1000 [==============================] - 0s 44us/step - loss: 2.3004 - acc: 0.1220
Epoch 4/10

  32/1000 [..............................] - ETA: 0s - loss: 2.2701 - acc: 0.1250
1000/1000 [==============================] - 0s 45us/step - loss: 2.2880 - acc: 0.1330
Epoch 5/10

  32/1000 [..............................] - ETA: 0s - loss: 2.2499 - acc: 0.1250
1000/1000 [==============================] - 0s 45us/step - loss: 2.2774 - acc: 0.1430
Epoch 6/10

  32/1000 [..............................] - ETA: 0s - loss: 2.2757 - acc: 0.1875
1000/1000 [==============================] - 0s 44us/step - loss: 2.2675 - acc: 0.1610
Epoch 7/10

  32/1000 [..............................] - ETA: 0s - loss: 2.2787 - acc: 0.1250
1000/1000 [==============================] - 0s 44us/step - loss: 2.2610 - acc: 0.1550
Epoch 8/10

  32/1000 [..............................] - ETA: 0s - loss: 2.3345 - acc: 0.1875
1000/1000 [==============================] - 0s 43us/step - loss: 2.2502 - acc: 0.1610
Epoch 9/10

  32/1000 [..............................] - ETA: 0s - loss: 2.2175 - acc: 0.1875
1000/1000 [==============================] - 0s 43us/step - loss: 2.2404 - acc: 0.1760
Epoch 10/10

  32/1000 [..............................] - ETA: 0s - loss: 2.2746 - acc: 0.1562
1000/1000 [==============================] - 0s 43us/step - loss: 2.2314 - acc: 0.1760
#+end_example

